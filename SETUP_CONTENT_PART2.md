# Setup Content - Part 2: Scripts, Source Code, and Evaluation

## `scripts/generate_context_map.py`

```python
#!/usr/bin/env python3
"""
Purpose: Parses src/ directory and generates CODE_CONTEXT.md with modules and functions.

Role: Maintenance script run after adding new code to keep the AI's context map up-to-date.

Assumptions:
- All code follows DOC_STANDARDS.md (DSPy-idiomatic)
- Extracts dspy.Module docstrings (human/AI context)
- IGNORES dspy.Signature docstrings (functional LLM instructions)
- The src/ directory contains the main application code
- Output is written to .project_dev/CODE_CONTEXT.md
"""

import ast
import os
from datetime import datetime
from pathlib import Path


def extract_docstring(node):
    """Extract docstring from an AST node."""
    return ast.get_docstring(node) or "No docstring provided."


def parse_python_file(filepath):
    """Parse a Python file and extract classes and functions."""
    with open(filepath, 'r', encoding='utf-8') as f:
        try:
            tree = ast.parse(f.read(), filename=str(filepath))
        except SyntaxError:
            return None
    
    modules = []
    signatures = []
    functions = []
    
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            docstring = extract_docstring(node)
            bases = [base.id for base in node.bases if isinstance(base, ast.Name)]
            
            if 'Module' in bases or 'dspy.Module' in str(bases):
                modules.append({
                    'name': node.name,
                    'docstring': docstring,
                    'file': filepath
                })
            elif 'Signature' in bases or 'dspy.Signature' in str(bases):
                signatures.append({
                    'name': node.name,
                    'docstring': docstring,
                    'file': filepath
                })
        
        elif isinstance(node, ast.FunctionDef):
            if not node.name.startswith('_'):  # Skip private functions
                docstring = extract_docstring(node)
                args = [arg.arg for arg in node.args.args]
                functions.append({
                    'name': node.name,
                    'args': args,
                    'docstring': docstring,
                    'file': filepath
                })
    
    return {'modules': modules, 'signatures': signatures, 'functions': functions}


def generate_context_map(src_dir='src', output_file='.project_dev/CODE_CONTEXT.md'):
    """Generate the CODE_CONTEXT.md file."""
    src_path = Path(src_dir)
    
    all_modules = []
    all_signatures = []
    all_functions = []
    
    # Walk through all Python files in src/
    for py_file in src_path.rglob('*.py'):
        if py_file.name == '__init__.py':
            continue
        
        result = parse_python_file(py_file)
        if result:
            all_modules.extend(result['modules'])
            all_signatures.extend(result['signatures'])
            all_functions.extend(result['functions'])
    
    # Generate markdown content
    content = f"""# Code Context Map

**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

**Purpose:** This file provides a high-level map of all modules and key functions in the codebase. It is auto-generated by `scripts/generate_context_map.py`.

**Note:** Per DOC_STANDARDS.md, dspy.Signature docstrings are NOT included here (they are functional LLM instructions, not human/AI documentation).

---

## DSPy Modules

"""
    
    if all_modules:
        for module in all_modules:
            rel_path = os.path.relpath(module['file'])
            content += f"### `{module['name']}`\n\n"
            content += f"**File:** `{rel_path}`\n\n"
            content += f"```\n{module['docstring']}\n```\n\n"
    else:
        content += "*No modules defined yet.*\n\n"
    
    content += "---\n\n## DSPy Signatures (Names Only)\n\n"
    content += "*Signature docstrings are functional LLM instructions and are not included in this context map.*\n\n"
    
    if all_signatures:
        for sig in all_signatures:
            rel_path = os.path.relpath(sig['file'])
            content += f"- `{sig['name']}` in `{rel_path}`\n"
        content += "\n"
    else:
        content += "*No signatures defined yet.*\n\n"
    
    content += "---\n\n## Utility Functions\n\n"
    
    if all_functions:
        for func in all_functions:
            rel_path = os.path.relpath(func['file'])
            args_str = ', '.join(func['args'])
            content += f"### `{func['name']}({args_str})`\n\n"
            content += f"**File:** `{rel_path}`\n\n"
            content += f"```\n{func['docstring']}\n```\n\n"
    else:
        content += "*No utility functions defined yet.*\n\n"
    
    content += "---\n\n**Note:** This file is automatically regenerated. Do not edit manually.\n"
    
    # Write to output file
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"✓ Generated {output_file}")
    print(f"  - {len(all_modules)} modules (with full docstrings)")
    print(f"  - {len(all_signatures)} signatures (names only - docstrings are functional)")
    print(f"  - {len(all_functions)} functions (with full docstrings)")


if __name__ == '__main__':
    generate_context_map()
```

---

## `src/__init__.py`

```python
"""
DSPy Application Package

This package contains the main DSPy application modules, signatures, and pipeline.
"""

__version__ = "0.1.0"
```

---

## `src/modules/__init__.py`

```python
"""
DSPy Modules

This package contains all dspy.Module classes for the application.
"""
```

---

## `src/signatures/__init__.py`

```python
"""
DSPy Signatures

This package contains all dspy.Signature classes for the application.
"""
```

---

## `src/main_pipeline.py`

```python
"""
Purpose: Assembles and orchestrates the complete DSPy application pipeline.

Role: Entry point for the application, connecting all modules and signatures.

Assumptions:
- Configuration is loaded from config/*.json
- All required modules are initialized before pipeline execution
- Returns structured output compatible with evaluation harness
"""

import dspy
from typing import Dict, Any


class MainPipeline(dspy.Module):
    """
    Purpose: The main DSPy application pipeline.
    
    Role: Orchestrates all modules and signatures to process user input and generate output.
    
    Assumptions:
    - Initialized with a valid configuration dictionary
    - Input format: {"query": str, ...}
    - Output format: {"result": str, "metadata": dict}
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        # TODO: Initialize modules here
        # Example: self.rag_module = RAGModule(retriever=...)
    
    def forward(self, query: str) -> Dict[str, Any]:
        """
        Purpose: Process a query through the complete pipeline.
        
        Role: Main execution method called by the evaluation harness.
        
        Assumptions:
        - query is a non-empty string
        - Returns a dictionary with 'result' key
        """
        # TODO: Implement pipeline logic
        return {
            "result": "Pipeline not yet implemented",
            "metadata": {}
        }


def create_pipeline(config: Dict[str, Any]) -> MainPipeline:
    """
    Purpose: Factory function to create and configure the main pipeline.
    
    Role: Used by evaluate.py and application entry points.
    
    Assumptions:
    - config contains all required keys (llm, retriever, etc.)
    - LLM API keys are set in environment variables
    """
    # Configure DSPy LLM
    lm = dspy.OpenAI(
        model=config['llm']['model'],
        temperature=config['llm']['temperature'],
        max_tokens=config['llm']['max_tokens']
    )
    dspy.settings.configure(lm=lm)
    
    return MainPipeline(config)
```

---

## `evaluate.py`

```python
#!/usr/bin/env python3
"""
Purpose: The main test harness for evaluating the DSPy application against test datasets.

Role: Core evaluation script run for every experiment to measure performance.

Assumptions:
- Test data is in JSONL format with 'input' and 'expected_output' keys
- Configuration file contains all required settings
- Results are saved to a timestamped directory for version control
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import time

import dspy
from src.main_pipeline import create_pipeline


def load_test_data(filepath: str) -> List[Dict[str, Any]]:
    """Load test data from JSONL file."""
    examples = []
    with open(filepath, 'r') as f:
        for line in f:
            examples.append(json.loads(line))
    return examples


def load_config(filepath: str) -> Dict[str, Any]:
    """Load configuration from JSON file."""
    with open(filepath, 'r') as f:
        return json.load(f)


def calculate_metrics(results: List[Dict[str, Any]]) -> Dict[str, float]:
    """
    Purpose: Calculate evaluation metrics from results.
    
    Role: Computes accuracy, latency, and other metrics for the evaluation report.
    
    Assumptions:
    - results contains 'expected' and 'actual' keys
    - Latency is measured in seconds
    """
    if not results:
        return {}
    
    # Simple exact match accuracy
    correct = sum(1 for r in results if r.get('expected') == r.get('actual'))
    accuracy = correct / len(results) if results else 0.0
    
    # Average latency
    latencies = [r.get('latency', 0) for r in results]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0.0
    
    return {
        "accuracy": round(accuracy, 4),
        "total_examples": len(results),
        "correct": correct,
        "avg_latency_seconds": round(avg_latency, 4)
    }


def run_evaluation(pipeline, test_data: List[Dict[str, Any]], verbose: bool = False) -> List[Dict[str, Any]]:
    """
    Purpose: Run the pipeline against all test examples and collect results.
    
    Role: Main evaluation loop that processes each test case.
    
    Assumptions:
    - pipeline is a configured MainPipeline instance
    - test_data contains 'input' and 'expected_output' keys
    """
    results = []
    
    for i, example in enumerate(test_data):
        if verbose:
            print(f"Processing example {i+1}/{len(test_data)}...")
        
        start_time = time.time()
        try:
            output = pipeline(query=example['input'])
            actual = output.get('result', '')
        except Exception as e:
            actual = f"ERROR: {str(e)}"
        
        latency = time.time() - start_time
        
        result = {
            "input": example['input'],
            "expected": example.get('expected_output', ''),
            "actual": actual,
            "latency": latency
        }
        results.append(result)
        
        if verbose:
            print(f"  Expected: {result['expected']}")
            print(f"  Actual: {result['actual']}")
            print(f"  Latency: {latency:.3f}s\n")
    
    return results


def save_results(results: List[Dict[str, Any]], metrics: Dict[str, float], output_dir: str):
    """
    Purpose: Save evaluation results and metrics to version-controlled files.
    
    Role: Persists results for reproducibility and historical tracking.
    
    Assumptions:
    - output_dir will be created if it doesn't exist
    - Results are saved as JSONL and metrics as JSON
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Save results
    results_file = output_path / 'results.jsonl'
    with open(results_file, 'w') as f:
        for result in results:
            f.write(json.dumps(result) + '\n')
    
    # Save metrics
    metrics_file = output_path / 'metrics.json'
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\n✓ Results saved to {output_dir}")
    print(f"  - {results_file}")
    print(f"  - {metrics_file}")


def main():
    parser = argparse.ArgumentParser(description='Evaluate DSPy application')
    parser.add_argument('--config', required=True, help='Path to config JSON file')
    parser.add_argument('--test_set', required=True, help='Path to test data JSONL file')
    parser.add_argument('--output_dir', help='Directory to save results (default: results/eval_TIMESTAMP)')
    
    args = parser.parse_args()
    
    # Load configuration
    print(f"Loading configuration from {args.config}...")
    config = load_config(args.config)
    
    # Load test data
    print(f"Loading test data from {args.test_set}...")
    test_data = load_test_data(args.test_set)
    print(f"Loaded {len(test_data)} test examples")
    
    # Create pipeline
    print("Initializing pipeline...")
    pipeline = create_pipeline(config)
    
    # Run evaluation
    print("\nRunning evaluation...")
    verbose = config.get('evaluation', {}).get('verbose', False)
    results = run_evaluation(pipeline, test_data, verbose=verbose)
    
    # Calculate metrics
    print("\nCalculating metrics...")
    metrics = calculate_metrics(results)
    
    # Print metrics
    print("\n" + "="*50)
    print("EVALUATION RESULTS")
    print("="*50)
    for key, value in metrics.items():
        print(f"{key}: {value}")
    print("="*50)
    
    # Save results
    if args.output_dir:
        output_dir = args.output_dir
    else:
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        output_dir = f"results/eval_{timestamp}"
    
    save_results(results, metrics, output_dir)
    
    print("\n✓ Evaluation complete!")


if __name__ == '__main__':
    main()
```

---

## `README.md`

```markdown
# DSPy Project

## Overview

[Describe what this DSPy application does]

## Architecture

[High-level description of the pipeline]

## Setup

### Prerequisites

- Python 3.9+
- OpenAI API key (or other LLM provider)

### Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install dspy-ai openai

# Set up environment variables
export OPENAI_API_KEY="your-api-key-here"
```

### Initial Setup

```bash
# Generate the code context map
python scripts/generate_context_map.py
```

## Usage

### Running the Application

```bash
python src/main_pipeline.py
```

### Running Evaluations

```bash
# Quick test with dev set
python evaluate.py --config config/dev_config.json --test_set test_data/dev_set.jsonl

# Full evaluation with gold set
python evaluate.py \
  --config config/dev_config.json \
  --test_set test_data/gold_set.jsonl \
  --output_dir results/H-XXX_eval_$(date +%Y-%m-%d)
```

## Development Workflow

See `.project_dev/DEV_NORMS.md` for branching and commit standards.

See `.project_dev/TDD_PROCESS.md` for the test-driven development process.

See `HYPOTHESES.md` for the current R&D plan and experiment history.

## Project Structure

```
.
├── .project_dev/          # AI context and development norms
├── config/                # Configuration files
├── logs/                  # Local debug logs (gitignored)
├── results/               # Evaluation results (version controlled)
├── scripts/               # Automation scripts
├── src/                   # Application source code
├── test_data/             # Test datasets
├── evaluate.py            # Main evaluation harness
├── HYPOTHESES.md          # R&D hypothesis log
└── README.md              # This file
```

## Contributing

1. Create a new hypothesis in `HYPOTHESES.md`
2. Create a feature branch: `feature/H-XXX-description`
3. Follow TDD process in `.project_dev/TDD_PROCESS.md`
4. Run full evaluation and commit results
5. Update hypothesis with outcomes
6. Create pull request with test results

## License

[Your license here]
```
